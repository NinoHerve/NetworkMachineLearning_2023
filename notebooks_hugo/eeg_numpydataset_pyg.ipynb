{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from utils.dataset import EEGDataset\n",
    "from torcheeg.datasets import NumpyDataset\n",
    "from torcheeg import transforms\n",
    "from torcheeg.transforms.pyg import ToG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to eeg dataset\n",
    "eeg_dir  = Path('../EEGDataset')\n",
    "\n",
    "# subjects\n",
    "#subjects = ['sub-01', 'sub-02', 'sub-03', 'sub-04']\n",
    "subjects = ['sub-01']\n",
    "\n",
    "# dataset using only selected subjects\n",
    "dataset = EEGDataset(eeg_dir, subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "588"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = []\n",
    "labels = []\n",
    "for f,_ in enumerate(dataset.files):\n",
    "    sample = dataset.__getitem__(f)\n",
    "    epochs.append(sample.get('eeg'))\n",
    "    labels.append(sample.get('label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X : (588, 128, 625)\n",
      "Shape of y : (588,)\n"
     ]
    }
   ],
   "source": [
    "X = np.stack(epochs, axis=0)\n",
    "y = np.stack(labels, axis=0)\n",
    "print('Shape of X : ' + str(X.shape))\n",
    "print('Shape of y : ' + str(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = {'trial_type':y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = np.load('../utils/electrodes_adj.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NUMPY]: 100%|██████████| 5/5 [00:10<00:00,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait for the writing process to complete...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = NumpyDataset(X=X,\n",
    "                       y=y,\n",
    "                       io_path = '../data_io_pyg_no_label_trans/',\n",
    "                       io_size=10485760*2,\n",
    "                       offline_transform=transforms.BandDifferentialEntropy(),\n",
    "                       online_transform=ToG(adj),\n",
    "                       label_transform=transforms.Select('trial_type'),                       \n",
    "                       num_worker=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheeg.datasets.constants.emotion_recognition.deap import DEAP_ADJACENCY_MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tog_deap = ToG(DEAP_ADJACENCY_MATRIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 128], x=[32, 128], edge_weight=[128])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tog_deap(eeg=np.random.randn(32, 128))['eeg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tog_trans = ToG(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eeg': Data(edge_index=[2, 686], x=[32, 128], edge_weight=[686])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tog_trans(eeg=np.random.randn(32, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheeg import model_selection\n",
    "train_dataset, val_dataset = model_selection.train_test_split(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "558"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Data(edge_index=[2, 686], x=[128, 4], edge_weight=[686]), 0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 686], x=[128, 4], edge_weight=[686])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheeg.model_selection import KFold\n",
    "\n",
    "k_fold = KFold(n_splits=5,\n",
    "               split_path=f'./tmp_out/split',\n",
    "               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, in_channels=4, num_layers=3, hid_channels=64, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(in_channels, hid_channels)\n",
    "        self.convs = nn.ModuleList()\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(GATConv(hid_channels, hid_channels))\n",
    "        self.lin1 = nn.Linear(hid_channels, hid_channels)\n",
    "        self.lin2 = nn.Linear(hid_channels, num_classes)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        self.lin1.reset_parameters()\n",
    "        self.lin2.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        for conv in self.convs:\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device = 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        X = batch[0].to(device)\n",
    "        y = batch[1].to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            loss, current = loss.item(), batch_idx * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def valid(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X = batch[0].to(device)\n",
    "            y = batch[1].to(device)\n",
    "\n",
    "            pred = model(X)\n",
    "            val_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    val_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {val_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.662676  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.747202 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.726179  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.737512 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.673674  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.731270 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.694783  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.727810 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.687811  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.723498 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.719274  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.717246 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.701333  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.715232 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.686358  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.715789 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.687508  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.719772 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.680514  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.724253 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.666032  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.726944 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.683985  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.724837 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.673011  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.721934 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.663727  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.721175 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.697110  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.719886 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.686380  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.721016 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.718815  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.723758 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.728721  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.725972 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.679472  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.726574 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.691711  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.725907 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.689933  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.726795 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.702891  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.728057 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.696210  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.724342 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.680819  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.724440 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.706283  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.723488 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.712326  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.721581 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.661646  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.724269 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.695339  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.725516 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.686618  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.725411 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.683815  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.723454 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.704827  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.721180 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.686358  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.721407 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.685021  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.719931 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.688500  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.719908 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.691021  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.720529 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.697896  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.721849 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.692768  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.723832 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.707749  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.723621 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.679265  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.724331 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.693760  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.722977 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.691570  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.722650 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.682446  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.723390 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.705616  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.722658 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.692954  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.722855 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.725038  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.722434 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.707277  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.723163 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.700342  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.722746 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.692118  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.723369 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.699674  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.722858 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.694573  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 0.723418 \n",
      "\n",
      "Done!\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.722148  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.722323 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.688967  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.715520 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.689612  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.709649 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.688009  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.707425 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.683643  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.706034 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.705263  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.704286 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.704117  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.702461 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.700424  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.702373 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.670415  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.703998 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.697933  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.705737 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.706676  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.705095 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.693111  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.706304 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.713829  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.705729 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.686507  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.705606 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.684258  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.706698 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.699161  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.705030 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.704260  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.703602 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.711742  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.701757 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.673431  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.701312 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.684184  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.700678 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.686315  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.702137 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.699928  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.703195 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.680422  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.703118 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.700988  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.702902 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.710587  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.703628 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.693974  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.703214 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.683416  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.703534 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.697440  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.703565 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.704443  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.703429 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.708257  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.701874 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.690798  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.702785 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.694587  [    0/  470]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/hugofluhr/Documents/Cours/NML/NetworkMachineLearning_2023/notebooks_hugo/eeg_numpydataset_pyg.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hugofluhr/Documents/Cours/NML/NetworkMachineLearning_2023/notebooks_hugo/eeg_numpydataset_pyg.ipynb#X30sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hugofluhr/Documents/Cours/NML/NetworkMachineLearning_2023/notebooks_hugo/eeg_numpydataset_pyg.ipynb#X30sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hugofluhr/Documents/Cours/NML/NetworkMachineLearning_2023/notebooks_hugo/eeg_numpydataset_pyg.ipynb#X30sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     train(train_loader, model, loss_fn, optimizer)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hugofluhr/Documents/Cours/NML/NetworkMachineLearning_2023/notebooks_hugo/eeg_numpydataset_pyg.ipynb#X30sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     valid(val_loader, model, loss_fn)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hugofluhr/Documents/Cours/NML/NetworkMachineLearning_2023/notebooks_hugo/eeg_numpydataset_pyg.ipynb#X30sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDone!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/hugofluhr/Documents/Cours/NML/NetworkMachineLearning_2023/notebooks_hugo/eeg_numpydataset_pyg.ipynb Cell 22\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hugofluhr/Documents/Cours/NML/NetworkMachineLearning_2023/notebooks_hugo/eeg_numpydataset_pyg.ipynb#X30sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Backpropagation\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hugofluhr/Documents/Cours/NML/NetworkMachineLearning_2023/notebooks_hugo/eeg_numpydataset_pyg.ipynb#X30sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hugofluhr/Documents/Cours/NML/NetworkMachineLearning_2023/notebooks_hugo/eeg_numpydataset_pyg.ipynb#X30sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hugofluhr/Documents/Cours/NML/NetworkMachineLearning_2023/notebooks_hugo/eeg_numpydataset_pyg.ipynb#X30sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hugofluhr/Documents/Cours/NML/NetworkMachineLearning_2023/notebooks_hugo/eeg_numpydataset_pyg.ipynb#X30sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mif\u001b[39;00m batch_idx \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "for i, (train_dataset, val_dataset) in enumerate(k_fold.split(dataset)):\n",
    "    model = GNN().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    epochs = 50\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(train_loader, model, loss_fn, optimizer)\n",
    "        valid(val_loader, model, loss_fn)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.704264  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.714069 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.701165  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.713987 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.709305  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.713842 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.692488  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.713500 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.691618  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.713353 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.662976  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.713038 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.710134  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.712622 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.701421  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.712221 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.689844  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.711714 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.702789  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.711466 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.700694  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.711105 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.694110  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.711030 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.670548  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.710991 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.722549  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.710754 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.704465  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.710457 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.672944  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.710469 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.679695  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.710388 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.713682  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.710301 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.715242  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.710134 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.704653  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.710000 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.717493  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.709622 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.712912  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.709288 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.700896  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.709217 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.717881  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.709085 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.697600  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.708992 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.686650  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.708566 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.704416  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.708230 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.697508  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.708257 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.712318  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.708274 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.684988  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.708293 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.700315  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.708352 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.688106  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.708461 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.671672  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.708159 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.695717  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.708264 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.707890  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.708404 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.707268  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.708624 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.692022  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.708754 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.698401  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.708722 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.689191  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.708638 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.704179  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.708297 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.677683  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.708385 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.685252  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.707986 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.676011  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.707622 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.682094  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.707446 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.691937  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.707454 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.682509  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.707428 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.683224  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.707426 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.686193  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.707516 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.688339  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.707436 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.693474  [    0/  470]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.707430 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torcheeg import model_selection\n",
    "train_dataset, val_dataset = model_selection.train_test_split(dataset)\n",
    "device = 'cpu'\n",
    "model = GNN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "epochs = 50\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, loss_fn, optimizer)\n",
    "    valid(val_loader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
