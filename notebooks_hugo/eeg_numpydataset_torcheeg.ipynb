{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../')\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from utils.dataset import EEGDataset\n",
    "\n",
    "from torcheeg.datasets import NumpyDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torcheeg.models import TSCeption\n",
    "from torcheeg.trainers import ClassificationTrainer\n",
    "import torchmetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to eeg dataset\n",
    "eeg_dir  = Path('../EEGDataset')\n",
    "\n",
    "# subjects\n",
    "#subjects = ['sub-01', 'sub-02', 'sub-03', 'sub-04']\n",
    "subjects = ['sub-01']\n",
    "\n",
    "# dataset using only selected subjects\n",
    "dataset = EEGDataset(eeg_dir, subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "588"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = []\n",
    "labels = []\n",
    "for f,_ in enumerate(dataset.files):\n",
    "    sample = dataset.__getitem__(f)\n",
    "    epochs.append(sample.get('eeg'))\n",
    "    labels.append(sample.get('label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X : (588, 128, 625)\n",
      "Shape of y : (588,)\n"
     ]
    }
   ],
   "source": [
    "X = np.stack(epochs, axis=0)\n",
    "y = np.stack(labels, axis=0)\n",
    "print('Shape of X : ' + str(X.shape))\n",
    "print('Shape of y : ' + str(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = {'trial_type':y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheeg import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The target folder already exists, if you need to regenerate the database IO, please delete the path ../data_io/.\n"
     ]
    }
   ],
   "source": [
    "dataset = NumpyDataset(X=X,\n",
    "                    y=y,\n",
    "                    io_path = '../data_io/',\n",
    "                    io_size=10485760*2,\n",
    "                    offline_transform=transforms.Compose([transforms.MeanStdNormalize(),\n",
    "                                                            transforms.To2d()]),\n",
    "                    online_transform=transforms.ToTensor(),\n",
    "                    label_transform=transforms.Select('trial_type'),           \n",
    "                    num_worker=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheeg.model_selection import KFold\n",
    "\n",
    "k_fold = KFold(n_splits=5,\n",
    "               split_path=f'./tmp_out/split',\n",
    "               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./tmp_out/examples_seed_tsception/log', exist_ok=True)\n",
    "logger = logging.getLogger('TSCeption with the SEED Dataset')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "console_handler = logging.StreamHandler()\n",
    "timeticks = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "file_handler = logging.FileHandler(os.path.join('./tmp_out/examples_seed_tsception/log', f'{timeticks}.log'))\n",
    "logger.addHandler(console_handler)\n",
    "logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClassificationTrainer(ClassificationTrainer):\n",
    "    def log(self, *args, **kwargs):\n",
    "        if self.is_main:\n",
    "            logger.info(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.694761, accuracy: 50.8% [    0/    2]\n",
      "loss: 0.694761, accuracy: 50.8% [    0/    2]\n",
      "loss: 0.685294, accuracy: 54.7% [    1/    2]\n",
      "loss: 0.685294, accuracy: 54.7% [    1/    2]\n",
      "\n",
      "loss: 0.682961, accuracy: 61.0%\n",
      "\n",
      "loss: 0.682961, accuracy: 61.0%\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.697523, accuracy: 48.0% [    0/    2]\n",
      "loss: 0.697523, accuracy: 48.0% [    0/    2]\n",
      "loss: 0.700552, accuracy: 43.9% [    1/    2]\n",
      "loss: 0.700552, accuracy: 43.9% [    1/    2]\n",
      "\n",
      "loss: 0.683217, accuracy: 61.0%\n",
      "\n",
      "loss: 0.683217, accuracy: 61.0%\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.688518, accuracy: 55.5% [    0/    2]\n",
      "loss: 0.688518, accuracy: 55.5% [    0/    2]\n",
      "loss: 0.691086, accuracy: 50.5% [    1/    2]\n",
      "loss: 0.691086, accuracy: 50.5% [    1/    2]\n",
      "\n",
      "loss: 0.683639, accuracy: 61.0%\n",
      "\n",
      "loss: 0.683639, accuracy: 61.0%\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.693248, accuracy: 53.1% [    0/    2]\n",
      "loss: 0.693248, accuracy: 53.1% [    0/    2]\n",
      "loss: 0.682968, accuracy: 54.7% [    1/    2]\n",
      "loss: 0.682968, accuracy: 54.7% [    1/    2]\n",
      "\n",
      "loss: 0.684008, accuracy: 61.0%\n",
      "\n",
      "loss: 0.684008, accuracy: 61.0%\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.684903, accuracy: 53.1% [    0/    2]\n",
      "loss: 0.684903, accuracy: 53.1% [    0/    2]\n",
      "loss: 0.682272, accuracy: 55.1% [    1/    2]\n",
      "loss: 0.682272, accuracy: 55.1% [    1/    2]\n",
      "\n",
      "loss: 0.684796, accuracy: 61.0%\n",
      "\n",
      "loss: 0.684796, accuracy: 61.0%\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.683834, accuracy: 57.0% [    0/    2]\n",
      "loss: 0.683834, accuracy: 57.0% [    0/    2]\n",
      "loss: 0.681871, accuracy: 56.1% [    1/    2]\n",
      "loss: 0.681871, accuracy: 56.1% [    1/    2]\n",
      "\n",
      "loss: 0.685429, accuracy: 61.0%\n",
      "\n",
      "loss: 0.685429, accuracy: 61.0%\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.679954, accuracy: 57.8% [    0/    2]\n",
      "loss: 0.679954, accuracy: 57.8% [    0/    2]\n",
      "loss: 0.676091, accuracy: 59.8% [    1/    2]\n",
      "loss: 0.676091, accuracy: 59.8% [    1/    2]\n",
      "\n",
      "loss: 0.685906, accuracy: 61.0%\n",
      "\n",
      "loss: 0.685906, accuracy: 61.0%\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.677957, accuracy: 55.1% [    0/    2]\n",
      "loss: 0.677957, accuracy: 55.1% [    0/    2]\n",
      "loss: 0.690790, accuracy: 51.9% [    1/    2]\n",
      "loss: 0.690790, accuracy: 51.9% [    1/    2]\n",
      "\n",
      "loss: 0.686084, accuracy: 62.7%\n",
      "\n",
      "loss: 0.686084, accuracy: 62.7%\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.675946, accuracy: 57.8% [    0/    2]\n",
      "loss: 0.675946, accuracy: 57.8% [    0/    2]\n",
      "loss: 0.676555, accuracy: 58.9% [    1/    2]\n",
      "loss: 0.676555, accuracy: 58.9% [    1/    2]\n",
      "\n",
      "loss: 0.685885, accuracy: 62.7%\n",
      "\n",
      "loss: 0.685885, accuracy: 62.7%\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.669468, accuracy: 60.2% [    0/    2]\n",
      "loss: 0.669468, accuracy: 60.2% [    0/    2]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/hugofluhr/Documents/Cours/NML/NetworkMachineLearning_2023/notebooks_hugo/eeg_numpydataset_torcheeg.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hugofluhr/Documents/Cours/NML/NetworkMachineLearning_2023/notebooks_hugo/eeg_numpydataset_torcheeg.ipynb#X25sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m val_loader \u001b[39m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hugofluhr/Documents/Cours/NML/NetworkMachineLearning_2023/notebooks_hugo/eeg_numpydataset_torcheeg.ipynb#X25sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# Do 50 rounds of training\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hugofluhr/Documents/Cours/NML/NetworkMachineLearning_2023/notebooks_hugo/eeg_numpydataset_torcheeg.ipynb#X25sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(train_loader, val_loader, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hugofluhr/Documents/Cours/NML/NetworkMachineLearning_2023/notebooks_hugo/eeg_numpydataset_torcheeg.ipynb#X25sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m trainer\u001b[39m.\u001b[39mtest(val_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hugofluhr/Documents/Cours/NML/NetworkMachineLearning_2023/notebooks_hugo/eeg_numpydataset_torcheeg.ipynb#X25sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m trainer\u001b[39m.\u001b[39msave_state_dict(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./tmp_out/examples_seed_tsception/weight/\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m.pth\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torcheeg/trainers/classification_trainer.py:223\u001b[0m, in \u001b[0;36mClassificationTrainer.fit\u001b[0;34m(self, train_loader, val_loader, num_epochs, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m    213\u001b[0m         train_loader: DataLoader,\n\u001b[1;32m    214\u001b[0m         val_loader: DataLoader,\n\u001b[1;32m    215\u001b[0m         num_epochs: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[1;32m    216\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    217\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[39m        train_loader (DataLoader): Iterable DataLoader for traversing the training data batch (torch.utils.data.dataloader.DataLoader, torch_geometric.loader.DataLoader, etc).\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[39m        val_loader (DataLoader): Iterable DataLoader for traversing the validation data batch (torch.utils.data.dataloader.DataLoader, torch_geometric.loader.DataLoader, etc).\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[39m        num_epochs (int): training epochs. (defualt: :obj:`1`)\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(train_loader\u001b[39m=\u001b[39;49mtrain_loader,\n\u001b[1;32m    224\u001b[0m                 val_loader\u001b[39m=\u001b[39;49mval_loader,\n\u001b[1;32m    225\u001b[0m                 num_epochs\u001b[39m=\u001b[39;49mnum_epochs,\n\u001b[1;32m    226\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torcheeg/trainers/basic_trainer.py:258\u001b[0m, in \u001b[0;36mBasicTrainer.fit\u001b[0;34m(self, train_loader, val_loader, num_epochs, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbefore_training_step(batch_id, num_batches, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    257\u001b[0m \u001b[39m# hook\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_training_step(train_batch, batch_id, num_batches,\n\u001b[1;32m    259\u001b[0m                       \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    260\u001b[0m \u001b[39m# hook\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter_training_step(batch_id, num_batches, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torcheeg/trainers/classification_trainer.py:148\u001b[0m, in \u001b[0;36mClassificationTrainer.on_training_step\u001b[0;34m(self, train_batch, batch_id, num_batches, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39m# backpropagation\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 148\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    149\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    151\u001b[0m \u001b[39m# log five times\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, (train_dataset, val_dataset) in enumerate(k_fold.split(dataset)):\n",
    "    # Initialize the model\n",
    "    model = TSCeption(num_electrodes=128,\n",
    "                      num_classes=2,\n",
    "                      num_T=15,\n",
    "                      num_S=15,\n",
    "                      in_channels=1,\n",
    "                      hid_channels=32,\n",
    "                      sampling_rate=128,\n",
    "                      dropout=0.5)\n",
    "\n",
    "    # Initialize the trainer and use the 0-th GPU for training\n",
    "    trainer = MyClassificationTrainer(model=model, lr=1e-4, weight_decay=1e-4)\n",
    "    # weird brute force stuff to put everything on MPS post-hoc\n",
    "    for k, m in trainer.modules.items():\n",
    "                trainer.modules[k] = m.to('cpu')\n",
    "    trainer.device = 'cpu'\n",
    "    trainer.train_loss.to(trainer.device)\n",
    "    trainer.train_accuracy.to(trainer.device)\n",
    "    trainer.val_loss.to(trainer.device)\n",
    "    trainer.val_accuracy.to(trainer.device)\n",
    "    trainer.test_loss.to(trainer.device)\n",
    "    trainer.test_accuracy.to(trainer.device)\n",
    "\n",
    "    # Initialize several batches of training samples and test samples\n",
    "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=10)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=10)\n",
    "\n",
    "    # Do 50 rounds of training\n",
    "    trainer.fit(train_loader, val_loader, num_epochs=50)\n",
    "    trainer.test(val_loader)\n",
    "    trainer.save_state_dict(f'./tmp_out/examples_seed_tsception/weight/{i}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DEAPDataset(io_path=f'./deap',\n",
    "            root_path='./data_preprocessed_python',\n",
    "            online_transform=transforms.ToTensor(),\n",
    "            label_transform=transforms.Compose([\n",
    "                transforms.Select('valence'),\n",
    "                transforms.Binary(5.0),\n",
    "            ]))\n",
    "model = GRU(num_electrodes=32, hid_channels=64, num_classes=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
